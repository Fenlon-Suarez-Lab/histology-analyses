{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-uniform temporal scaling of developmental processes in mammalian cortex\n",
    "#### Paolino et al. 2023\n",
    "\n",
    "This code corresponds to the compositional statistical tests used in Paolino et al. 2023 submitted for review to Nature Communications. Other statistical tests are not described here, as they used external Python and R scripts detailed in the manuscript (see Methods) and reported in the Supplementary Statistics Table 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math \n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import zscore\n",
    "from sklearn.utils import resample\n",
    "from skbio.stats.composition import clr, ilr\n",
    "from statsmodels.multivariate.manova import MANOVA\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from the master CSV file.\n",
    "sheet = 'name_of_relevant_excel_tab' # Change this to the name of the tab in the master Excel file.\n",
    "comp_df = pd.read_excel('Paolino2023_master_data.xlsx', sheet_name=sheet)\n",
    "comp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid errors working with 0 percentage/proportion values, a small value is added to all zeros and evenly subtracted from the non-zero values\n",
    "def adjust_values(series):\n",
    "    # Define the small value to add\n",
    "    small_value = 0.0001\n",
    "    # Find the zero values\n",
    "    zero_values = series == 0\n",
    "    # Add the small value to the zero values\n",
    "    series[zero_values] += small_value\n",
    "    # Calculate the total added amount\n",
    "    total_added = small_value * zero_values.sum()\n",
    "    # Check if there are any non-zero values\n",
    "    if (~zero_values).sum() > 0:\n",
    "        # Subtract the total added amount evenly from the non-zero values\n",
    "        series[~zero_values] -= total_added / (~zero_values).sum()\n",
    "    return series\n",
    "\n",
    "# Apply the function to each group of your DataFrame\n",
    "# Assuming 'group_column' is the name of the column you want to group by\n",
    "comp_df['prop_cells_reg'] = comp_df.groupby(['Species','Cortex'])['Prop_Ctx'].transform(adjust_values)\n",
    "comp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isometric log ratio transforms calculated from the proportions.\n",
    "ilr_dfs = {'Species':[],'Cortex': [],'ILR_p1':[],'ILR_p2':[],'ILR_p3':[],'ILR_p4':[]}\n",
    "\n",
    "for name, data in comp_df.groupby(['Species','Cortex']):\n",
    "    ilr_dfs['Species'].append(name[0])\n",
    "    ilr_dfs['Cortex'].append(name[1])\n",
    "    prop_by_ctxandlayer = data['prop_cells_reg'].values\n",
    "    print(name, np.sum(prop_by_ctxandlayer))\n",
    "    ilr_prop_by_ctxandlayer = ilr(prop_by_ctxandlayer)\n",
    "    # m - 1 the number of parts in the composition, could be more or less depending on specific comparison being made.\n",
    "    ilr_dfs['ILR_p1'].append(ilr_prop_by_ctxandlayer[0])\n",
    "    ilr_dfs['ILR_p2'].append(ilr_prop_by_ctxandlayer[1])\n",
    "    ilr_dfs['ILR_p3'].append(ilr_prop_by_ctxandlayer[2])\n",
    "    ilr_dfs['ILR_p4'].append(ilr_prop_by_ctxandlayer[3])\n",
    "\n",
    "# Convert to a pandas dataframe\n",
    "ilr_dfs = pd.DataFrame(ilr_dfs)\n",
    "ilr_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we calculate residuals and add them to the dataframe. \n",
    "ilr_dfs['p1_residuals'] = ols('ILR_p1 ~ C(Species)', data=ilr_dfs).fit().resid\n",
    "ilr_dfs['p2_residuals'] = ols('ILR_p2 ~ C(Species)', data=ilr_dfs).fit().resid\n",
    "ilr_dfs['p3_residuals'] = ols('ILR_p3 ~ C(Species)', data=ilr_dfs).fit().resid\n",
    "ilr_dfs['p4_residuals'] = ols('ILR_p4 ~ C(Species)', data=ilr_dfs).fit().resid  \n",
    "ilr_dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of assumptions tests for multivariate normality and homogeneity of covariance matrices. \n",
    "pg.multivariate_normality(ilr_dfs[['p1_residuals','p2_residuals','p3_residuals','p4_residuals']])\n",
    "pg.box_m(ilr_dfs, dvs=['p1_residuals','p2_residuals','p3_residuals','p4_residuals'], group='Species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For parametric tests, we use MANOVA or switch to a non-parametric test in R (see Methods).\n",
    "maov = MANOVA.from_formula('ILR_p1 + ILR_p2 + ILR_p3 + ILR_p4 ~ Species', data=ilr_dfs)\n",
    "print(maov.mv_test())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log ratio of geometric means and plot the result.\n",
    "\n",
    "def calc_log_ratio(data):\n",
    "    # Resample the data with replacement\n",
    "    boot_data = data.sample(n=len(data), replace=True)\n",
    "    \n",
    "    # Split the data by condition\n",
    "    Mm = boot_data[boot_data['Species'] == 'Ms']\n",
    "    Sc = boot_data[boot_data['Species'] == 'FTD']\n",
    "    \n",
    "    # Calculate geometric means\n",
    "    gm_mm = np.exp(np.mean(np.log(Mm['prop_cells_reg'])))\n",
    "    gm_sc = np.exp(np.mean(np.log(Sc['prop_cells_reg'])))\n",
    "\n",
    "    return np.log(gm_mm/gm_sc)\n",
    "\n",
    "# Apply the bootstrapping function for each layer\n",
    "results = {}\n",
    "for layer in comp_df['Layer'].unique(): # Loop through each layer\n",
    "    layer_data = comp_df[comp_df['Layer'] == layer] # Get data for the layer\n",
    "    print(layer_data)\n",
    "    boot_results = [calc_log_ratio(layer_data) for _ in range(5000)] # Run bootstrapping\n",
    "    results[layer] = boot_results # Store results in dictionary\n",
    "\n",
    "# Convert results to DataFrame and drop rows with NaN values\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.dropna()\n",
    "\n",
    "# Calculate means for each layer\n",
    "means = results_df.mean()\n",
    "\n",
    "# Calculate 95% confidence intervals for each layer\n",
    "ci_lower = results_df.apply(lambda x: np.percentile(x, 2.5))\n",
    "ci_upper = results_df.apply(lambda x: np.percentile(x, 97.5))\n",
    "\n",
    "# Calculate 99% confidence intervals for each layer\n",
    "# ci_lower = results_df.apply(lambda x: np.percentile(x, 0.5))\n",
    "# ci_upper = results_df.apply(lambda x: np.percentile(x, 99.5))\n",
    "\n",
    "# Create bar plot with error bars\n",
    "plt.figure(figsize=(2,4))\n",
    "plt.bar(means.index, means.values, yerr=[means.values-ci_lower.values, ci_upper.values-means.values], capsize=4,color=['#009392','#EEB479','#E88471','#F2C6DE'])\n",
    "plt.xlabel('Staining')\n",
    "plt.ylabel('Log ratio (Mm/Sc)')\n",
    "plt.axhline(0,linestyle='--',color='black')\n",
    "plt.xticks(rotation=45)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
